{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mailythompson/BME3053C/blob/main/lessons-solved/unsupervised_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r92-oWa55alY"
      },
      "source": [
        "### Exploring Hidden Layer Representations in Neural Networks with PCA and t-SNE\n",
        "\n",
        "\n",
        "In this assignment, we will investigate how neural networks represent information in their hidden layers, comparing networks with trained versus untrained parameters. Using unsupervised learning techniques, specifically Principal Component Analysis (PCA) and t-SNE, we will visualize the hidden layer outputs in two dimensions.\n",
        "\n",
        "Through these visualizations, you will explore and analyze two key aspects:\n",
        "1. The distinctions between PCA and t-SNE as dimensionality reduction techniques.\n",
        "2. The differences in the learned representations of hidden layers before and after training.\n",
        "\n",
        "This analysis will help you gain insights into how training affects feature learning and dimensionality reduction. We will use the MNIST dataset for this task, but similar results can be observed with other datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llggnbD75alb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    from sklearn.datasets import fetch_openml\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    import plotly.express as px\n",
        "    from sklearn.manifold import TSNE\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.decomposition import PCA\n",
        "    import seaborn as sns\n",
        "except ImportError as e:\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    print(f\"Missing package: {str(e).split()[-1]}\")\n",
        "    print(\"Installing required packages...\")\n",
        "    %pip install scikit-learn\n",
        "    %pip install seaborn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoHp47qG5alc"
      },
      "source": [
        "##### We will use the MNIST dataset to compare representations in the hidden layers of a neural network. Follow these steps to load, scale, and split the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxGx9ZWt5alc",
        "outputId": "ed739618-949a-405f-e4b1-4102dfd6cdf5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'fetch_openml' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load MNIST dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X, y \u001b[38;5;241m=\u001b[39m fetch_openml(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist_784\u001b[39m\u001b[38;5;124m'\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, return_X_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, as_frame\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Convert X and y to pandas DataFrames\u001b[39;00m\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fetch_openml' is not defined"
          ]
        }
      ],
      "source": [
        "# Load MNIST dataset\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "# Convert X and y to pandas DataFrames\n",
        "X = pd.DataFrame(X, columns=[f'pixel_{i}' for i in range(X.shape[1])])\n",
        "y = pd.Series(y, name='label')\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHWcJaai5ald"
      },
      "source": [
        "##### The cell below defines a function that creates an MLP (Multi-Layer Perceptron) classifier with preset parameters.\n",
        "\n",
        "##### **You should use the `get_mlp_classifier()` function below to create the MLP classifiers for this homework.**\n",
        "\n",
        "The classifier is configured with:\n",
        "- One hidden layer of 100 neurons\n",
        "- ReLU activation function\n",
        "- Adam optimizer\n",
        "- 20 maximum iterations\n",
        "- Random state of 42 for reproducibility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIyk2ih05ald"
      },
      "outputs": [],
      "source": [
        "def get_mlp_classifier(max_iter=20):\n",
        "    return MLPClassifier(\n",
        "        hidden_layer_sizes=(200,),  # One hidden layer with 100 neurons\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        max_iter=max_iter,\n",
        "        random_state=42,\n",
        "        verbose=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXsctlHZ5ald"
      },
      "source": [
        "`get_untrained_hidden_layer_outputs` creates a new classifier and then computes the outputs of the hidden layer with the test set as an input.\n",
        "\n",
        "The outputs of a neural network hidden layer are intermediate representations of the input data\n",
        "after being transformed by the layer's weights and activation function. These outputs capture\n",
        "learned features and patterns from the input data.\n",
        "\n",
        "The resulting values represent abstract features that the network uses for its final prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpio1KgB5ald"
      },
      "outputs": [],
      "source": [
        "def get_untrained_hidden_layer_outputs():\n",
        "    mlp_classifier= get_mlp_classifier(1)\n",
        "    single_X=np.random.uniform(0, 1, size=(1, 784))\n",
        "    single_y=[1]\n",
        "    single_X = X_train[0:1]  # Take first sample\n",
        "    single_y = y_train[0:1]  # Take first label\n",
        "\n",
        "    mlp_classifier.partial_fit(single_X, single_y, classes=np.unique(y_train))\n",
        "\n",
        "\n",
        "\n",
        "    intermediate_output = X_test @ mlp_classifier.coefs_[0]\n",
        "    intermediate_output = np.maximum(0, intermediate_output)\n",
        "    return intermediate_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsz8EJ_n5ale"
      },
      "source": [
        "`get_trained_hidden_layer_outputs` creates a new classifier, trains it on the full training set, and then computes the outputs of the hidden layer with the test set as input.\n",
        "Similar to the untrained version, this function:\n",
        "1. Creates an MLPClassifier with default parameters (20 max iterations)\n",
        "2. Trains the classifier on X_train and y_train using fit()\n",
        "3. Computes the hidden layer outputs by:\n",
        "   - Multiplying input data (X_test) with the first layer weights (coefs_[0])\n",
        "   - Applying ReLU activation (max(0,x)) to the result\n",
        "The key difference is that this classifier is fully trained, so its hidden layer representations\n",
        "should be more meaningful for distinguishing between different digit classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGFY4QSD5ale"
      },
      "outputs": [],
      "source": [
        "def get_trained_hidden_layer_outputs():\n",
        "    mlp_classifier= get_mlp_classifier()\n",
        "    # mlp_classifier.fit(single_X, )\n",
        "    mlp_classifier.fit(X_train, y_train)\n",
        "\n",
        "    intermediate_output = X_test @ mlp_classifier.coefs_[0]\n",
        "    intermediate_output = np.maximum(0, intermediate_output)\n",
        "    return intermediate_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImyN0vc35ale"
      },
      "source": [
        "### **Part 1A**\n",
        "##### Extract the outputs from the hidden layer of an *untrained* MLPClassifier and apply t-SNE to the hidden layer outputs to represent the data in 2D\n",
        "\n",
        "1. get the outputs from `get_untrained_hidden_layer_outputs()` and apply t-SNE to visualize the results\n",
        "2. Create a scatter plot of the 2D t-SNE results\n",
        "3. Color code the points based on their class labels (`y_test.astype(int)`)\n",
        "4. Add a colorbar to identify different classes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwlrzEzx5ale"
      },
      "outputs": [],
      "source": [
        "#Provide your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5hj6qZ05ale"
      },
      "source": [
        "### **Part 1B**\n",
        "##### Now, extract the outputs from the hidden layer of an *untrained* MLPClassifier and apply PCA instead of t-SNE\n",
        "\n",
        "1. get the outputs from `get_untrained_hidden_layer_outputs()` and apply PCA to visualize the results\n",
        "2. Create a scatter plot of the 2D PCA results\n",
        "3. Color code the points based on their class labels (`y_test.astype(int)`)\n",
        "4. Add a colorbar to identify different classes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKboOd8l5ale"
      },
      "outputs": [],
      "source": [
        "#Provide your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfOVcKfo5alf"
      },
      "source": [
        "### **Part 1C**\n",
        "#### In your own words, describe the difference between lower dimensional representations when using PCA vs t-SNE. Provide your response in the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf6geSk05alf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6nujZ315alf"
      },
      "source": [
        "### **Part 2A**\n",
        "##### Extract the outputs from the hidden layer of a *trained* MLPClassifier and apply t-SNE to the hidden layer outputs to represent the data in 2D\n",
        "\n",
        "1. get the outputs from `get_trained_hidden_layer_outputs()` and apply t-SNE to visualize the results\n",
        "2. Create a scatter plot of the 2D t-SNE results\n",
        "3. Color code the points based on their class labels (`y_test.astype(int)`)\n",
        "4. Add a colorbar to identify different classes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OtxhbF25alf"
      },
      "outputs": [],
      "source": [
        "#Provide your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3jZ26JO5alf"
      },
      "source": [
        "### **Part 2B**\n",
        "##### Extract the outputs from the hidden layer of an *trained* MLPClassifier and apply PCA to the hidden layer outputs to represent the data in 2D\n",
        "\n",
        "1. get the outputs from `get_trained_hidden_layer_outputs()` and apply PCA to visualize the results\n",
        "2. Create a scatter plot of the 2D PCA results\n",
        "3. Color code the points based on their class labels (`y_test.astype(int)`)\n",
        "4. Add a colorbar to identify different classes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLPoGqXK5alf"
      },
      "outputs": [],
      "source": [
        "#Provide your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jur_9reT5alf"
      },
      "source": [
        "### **Part 2C**\n",
        "##### In your own words, describe the difference between lower dimensional representations when using PCA vs t-SNE on the trained hidden layer outputs. Provide your response in the cell below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjDIrC415alf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLSf-9Id5alf"
      },
      "source": [
        "### **Part 3**\n",
        "\n",
        "1. How do the visualizations differ between the trained and untrained hidden layer outputs? What might account for these differences?\n",
        "2. What differences do you observe between PCA and t-SNE? Why might certain patterns be more pronounced in one method compared to the other?\n",
        "3. Why might certain clusters or patterns emerge only in the trained model?\n",
        "\n",
        "Provide your response in the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I715TqqQ5alf"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}